{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/zchao/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from congress import Congress\n",
    "api_key = 'fxzKX1XVwgCBfElNZlya7fwBVdEFNBlGEadiAFxD'\n",
    "congress_api = Congress(api_key)\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_seq_items', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "import csv\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = [10, 5]\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set(font_scale=1.5)\n",
    "import numpy as np\n",
    "import importlib\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "import CleanData, SaveState\n",
    "importlib.reload(CleanData)\n",
    "importlib.reload(SaveState)\n",
    "from CleanData import clean_comment, get_bag_of_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get stopword list\n",
    "stop_words = stopwords.words('english')\n",
    "number_strs = ['one', 'two', 'three', 'four', 'five', 'six', 'seven',\\\n",
    "'eight', 'nine', 'ten']\n",
    "stop_words.extend(number_strs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tweet from members\n",
    "filename = '../data/115_Senators_during_term.csv'\n",
    "tweets_df = pd.read_csv(filename)\n",
    "tweets_df = tweets_df.head(200000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df = tweets_df.loc[tweets_df['full_text'].apply(type) == str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Doug Jones', 'Cory Booker', 'Lindsey Graham', 'Kamala Harris',\n",
       "       'Hatch', 'Luther Strange', 'Bernie Sanders',\n",
       "       'Shelley Moore Capito', 'Maggie Hassan', 'Thom Tillis',\n",
       "       'Jerry Moran', 'Tom Carper', 'Martin Heinrich', 'Pat Toomey',\n",
       "       'Bill Nelson', 'Mike Crapo', 'Mike Rounds', 'Todd Young',\n",
       "       'John Barrasso', 'Murphy', 'Steve Daines', 'John Thune',\n",
       "       'Susan Collins', 'Patty Murray', 'John McCain', 'Dianne Feinstein',\n",
       "       'Jack Reed', 'ChuckGrassley', 'Cortez Masto', 'McCaskill',\n",
       "       'Lamar Alexander', 'Pat Roberts', 'Ben Cardin', 'Ron Wyden',\n",
       "       'Dean Heller', 'Dick Durbin', 'Jeanne Shaheen',\n",
       "       'Sheldon Whitehouse', 'Tammy Duckworth', 'Tom Cotton',\n",
       "       'Bob Corker', 'Sherrod Brown', 'Tom Udall', 'Gary Peters',\n",
       "       'Ted Cruz', 'Johnny Isakson', 'James Lankford', 'McConnell',\n",
       "       'Mike Enzi', 'Jeff Flake', 'Kirsten Gillibrand', 'Bob Casey',\n",
       "       'Rubio', 'Angus King', 'Inhofe', 'Chris Van Hollen', 'Rob Portman',\n",
       "       'Thad Cochran', 'Richard Burr', 'Bob Menendez', 'John Boozman',\n",
       "       'Jon Tester', 'Mazie Hirono', 'Deb Fischer', 'Michael Bennet',\n",
       "       'Debbie Stabenow', 'Joe Manchin', 'Richard Blumenthal', 'Mike Lee',\n",
       "       'Elizabeth Warren', 'Ben Sasse', 'Brian Schatz', 'Jim Risch',\n",
       "       'Tammy Baldwin', 'Lisa Murkowski', 'David Perdue',\n",
       "       'Maria Cantwell', 'Cory Gardner', 'Bill Cassidy', 'Al Franken',\n",
       "       'Amy Klobuchar', 'Joe Donnelly', 'Roger Wicker', 'Chris Coons',\n",
       "       'Heidi Heitkamp', 'Joni Ernst', 'Mark Warner', 'John Cornyn',\n",
       "       'Patrick Leahy', 'Ron Johnson', 'Chuck Schumer', 'Jeff Merkley',\n",
       "       'Roy Blunt', 'John Kennedy', 'Richard Shelby', 'Ed Markey',\n",
       "       'Rand Paul', 'Tim Scott', 'John Hoeven', 'Dan Sullivan',\n",
       "       'Tim Kaine'], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df['user_name'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_train_users = ['Chuck Schumer', 'Elizabeth Warren', 'Bernie Sanders','Michael Bennet', \\\n",
    "                   'Cory Booker', 'Amy Klobuchar']\n",
    "rep_train_users = ['McConnell', 'Rubio', 'John McCain', 'Ted Cruz']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_users = tweets_df['user_name'].unique().tolist()\n",
    "for user in (dem_train_users + rep_train_users):\n",
    "    test_users.remove(user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of democratic training tweets is 15772\n",
      "No. of republican training tweets is 10386\n"
     ]
    }
   ],
   "source": [
    "print('No. of democratic training tweets is',\n",
    "tweets_df.loc[tweets_df['user_name'].isin(dem_train_users)].shape[0])\n",
    "print('No. of republican training tweets is',\n",
    "tweets_df.loc[tweets_df['user_name'].isin(rep_train_users)].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of testing tweets is 173514\n"
     ]
    }
   ],
   "source": [
    "print('No. of testing tweets is',\n",
    "tweets_df.loc[tweets_df['user_name'].isin(test_users)].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['tweet_id', 'full_text', 'created_at', 'user_id', 'screen_name',\n",
      "       'user_name', 'hashtags'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(tweets_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(199672, 2666)\n"
     ]
    }
   ],
   "source": [
    "tweets_df['clean_tweet'] = tweets_df['full_text'].apply(clean_comment)\n",
    "bag_of_words, vectorizer = get_bag_of_words(tweets_df['clean_tweet'],ngram_range=(1,3), min_df=0.001)\n",
    "print(bag_of_words.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>full_text</th>\n",
       "      <th>created_at</th>\n",
       "      <th>user_id</th>\n",
       "      <th>screen_name</th>\n",
       "      <th>user_name</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>clean_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>940601256560783400</td>\n",
       "      <td>rt ashparso gdouglasjones i voted dougjones dougjonesforsenate</td>\n",
       "      <td>Tue Dec 12 15:16:29 +0000 2017</td>\n",
       "      <td>239548513</td>\n",
       "      <td>DougJones</td>\n",
       "      <td>Doug Jones</td>\n",
       "      <td>[]</td>\n",
       "      <td>rt ashparso gdouglasjones i voted dougjones dougjonesforsenate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>940625117176041500</td>\n",
       "      <td>today let’s get everyone to vote  thank you joebiden</td>\n",
       "      <td>Tue Dec 12 16:51:18 +0000 2017</td>\n",
       "      <td>239548513</td>\n",
       "      <td>DougJones</td>\n",
       "      <td>Doug Jones</td>\n",
       "      <td>[]</td>\n",
       "      <td>today let’s get everyone to vote  thank you joebiden</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>939908592802648000</td>\n",
       "      <td>let us go to the polls to do our own work and remember that our life depends on it because it does  congresswoman sewell</td>\n",
       "      <td>Sun Dec 10 17:24:05 +0000 2017</td>\n",
       "      <td>239548513</td>\n",
       "      <td>DougJones</td>\n",
       "      <td>Doug Jones</td>\n",
       "      <td>[]</td>\n",
       "      <td>let us go to the polls to do our own work and remember that our life depends on it because it does  congresswoman sewell</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>940583699728527400</td>\n",
       "      <td>today rightsideofhistory go vote</td>\n",
       "      <td>Tue Dec 12 14:06:43 +0000 2017</td>\n",
       "      <td>239548513</td>\n",
       "      <td>DougJones</td>\n",
       "      <td>Doug Jones</td>\n",
       "      <td>['RightSideOfHistory']</td>\n",
       "      <td>today rightsideofhistory go vote</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>940606216786665500</td>\n",
       "      <td>rt aldotcom just in case you were wondering   alpolitics alsen</td>\n",
       "      <td>Tue Dec 12 15:36:12 +0000 2017</td>\n",
       "      <td>239548513</td>\n",
       "      <td>DougJones</td>\n",
       "      <td>Doug Jones</td>\n",
       "      <td>['alpolitics'; 'alsen']</td>\n",
       "      <td>rt aldotcom just in case you were wondering   alpolitics alsen</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id  \\\n",
       "0  940601256560783400   \n",
       "1  940625117176041500   \n",
       "2  939908592802648000   \n",
       "3  940583699728527400   \n",
       "4  940606216786665500   \n",
       "\n",
       "                                                                                                                   full_text  \\\n",
       "0  rt ashparso gdouglasjones i voted dougjones dougjonesforsenate                                                              \n",
       "1  today let’s get everyone to vote  thank you joebiden                                                                        \n",
       "2  let us go to the polls to do our own work and remember that our life depends on it because it does  congresswoman sewell    \n",
       "3  today rightsideofhistory go vote                                                                                            \n",
       "4  rt aldotcom just in case you were wondering   alpolitics alsen                                                              \n",
       "\n",
       "                       created_at    user_id screen_name   user_name  \\\n",
       "0  Tue Dec 12 15:16:29 +0000 2017  239548513  DougJones   Doug Jones   \n",
       "1  Tue Dec 12 16:51:18 +0000 2017  239548513  DougJones   Doug Jones   \n",
       "2  Sun Dec 10 17:24:05 +0000 2017  239548513  DougJones   Doug Jones   \n",
       "3  Tue Dec 12 14:06:43 +0000 2017  239548513  DougJones   Doug Jones   \n",
       "4  Tue Dec 12 15:36:12 +0000 2017  239548513  DougJones   Doug Jones   \n",
       "\n",
       "                  hashtags  \\\n",
       "0  []                        \n",
       "1  []                        \n",
       "2  []                        \n",
       "3  ['RightSideOfHistory']    \n",
       "4  ['alpolitics'; 'alsen']   \n",
       "\n",
       "                                                                                                                 clean_tweet  \n",
       "0  rt ashparso gdouglasjones i voted dougjones dougjonesforsenate                                                             \n",
       "1  today let’s get everyone to vote  thank you joebiden                                                                       \n",
       "2  let us go to the polls to do our own work and remember that our life depends on it because it does  congresswoman sewell   \n",
       "3  today rightsideofhistory go vote                                                                                           \n",
       "4  rt aldotcom just in case you were wondering   alpolitics alsen                                                             "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train binary multinomial Naive Bayes model\n",
    "from sklearn.naive_bayes import MultinomialNB, ComplementNB\n",
    "from sklearn import metrics\n",
    "def get_binary_NB_model(bag_of_words, df):\n",
    "    # Training data:\n",
    "    class1_words = bag_of_words[df['user_name'].isin(dem_train_users),:]\n",
    "    class2_words = bag_of_words[df['user_name'].isin(rep_train_users),:]\n",
    "    train_tweets = np.concatenate((class1_words,class2_words))\n",
    "    labels = np.concatenate((np.zeros(class1_words.shape[0]),np.ones(class2_words.shape[0])))\n",
    "    nb = ComplementNB()\n",
    "    nb.fit(train_tweets, labels)\n",
    "    # # Performance on training data\n",
    "    predictions = nb.predict(train_tweets)\n",
    "    print('Training Accuracy: ' + str(sum(labels==predictions)/len(labels)))\n",
    "    # Compute the error.\n",
    "    tn, fp, fn, tp = metrics.confusion_matrix(labels,predictions).ravel()\n",
    "    print(tn, fp, fn, tp)\n",
    "    return nb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate array with shape (26158, 2666) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-af2cc12f63e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Train a naive bayes model on tweets from specified training users from each party.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnb_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_binary_NB_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbag_of_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtweets_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-14-be0ce1a7cc10>\u001b[0m in \u001b[0;36mget_binary_NB_model\u001b[0;34m(bag_of_words, df)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass1_words\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass2_words\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mnb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mComplementNB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mnb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_tweets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0;31m# # Performance on training data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_tweets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/FlowPredict/env/lib/python3.6/site-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    611\u001b[0m         self.feature_count_ = np.zeros((n_effective_classes, n_features),\n\u001b[1;32m    612\u001b[0m                                        dtype=np.float64)\n\u001b[0;32m--> 613\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    614\u001b[0m         \u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_alpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    615\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_feature_log_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/FlowPredict/env/lib/python3.6/site-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36m_count\u001b[0;34m(self, X, Y)\u001b[0m\n\u001b[1;32m    817\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Input X must be non-negative\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_count_\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0msafe_sparse_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_count_\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_all_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_count_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/FlowPredict/env/lib/python3.6/site-packages/sklearn/utils/extmath.py\u001b[0m in \u001b[0;36msafe_sparse_dot\u001b[0;34m(a, b, dense_output)\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: Unable to allocate array with shape (26158, 2666) and data type float64"
     ]
    }
   ],
   "source": [
    "# Train a naive bayes model on tweets from specified training users from each party.\n",
    "nb_model = get_binary_NB_model(bag_of_words, tweets_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
